{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proud-herald",
   "metadata": {},
   "source": [
    "# Keyword Extraction Comparison\n",
    "\n",
    "This notebook contains a comparison of various Keyword Extraction (KWE) strategies applied to the scotch dataset. Each strategy is timed and its extracted keywords are retained.\n",
    "\n",
    "A pdf of this notebook is saved - I have too frequently accidentally lost results of time intensive cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personal-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from whiskynlp.Vectorizer import ListFeatureVectorizer\n",
    "from whiskynlp.GraphKeywordExtraction import GraphKE\n",
    "from whiskynlp.WhiskyLemmatizer import WhiskyLemmatizer\n",
    "from nltk import pos_tag\n",
    "import math\n",
    "import time\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "middle-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "external-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "level-diving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01999974250793457"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anonymous-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scotch-no-dupes.csv\")\n",
    "# Making corpus list to operate on\n",
    "df[\"All\"] = df[\"Nose\"] + \" \" + df[\"Palate\"] + \" \" + df[\"Finish\"]\n",
    "lst = GraphKE().makeCorpusList(df, \"All\")\n",
    "corp = GraphKE().makeCorpus(lst)\n",
    "whisky_stopwords = WhiskyLemmatizer().swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equal-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-objective",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "https://www.analyticsvidhya.com/blog/2020/11/words-that-matter-a-simple-guide-to-keyword-extraction-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "swiss-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on implementation from analyticsvidhya, adapted\n",
    "def wordTFScore(corpus, docs):\n",
    "    tf = {}\n",
    "    words = corpus.split()\n",
    "    n_words = len(words)\n",
    "    for word in words:\n",
    "        word.replace('.','')\n",
    "        if word not in whisky_stopwords:\n",
    "            if word in tf:\n",
    "                tf[word] += 1\n",
    "            else:\n",
    "                tf[word] = 1\n",
    "    tf.update(\n",
    "        (x, y/int(n_words)) for x, y in tf.items()\n",
    "    )\n",
    "    return tf\n",
    "\n",
    "def countDocs(word, docs):\n",
    "    final = [all([w in x for w in word]) for x in docs]\n",
    "    return int(len([docs[i] for i in range(0, len(final)) if final[i]]))\n",
    "\n",
    "def wordIDFScore(corpus, docs):\n",
    "    idf = {}\n",
    "    words = corpus.split()\n",
    "    n_words = len(words)\n",
    "    n_docs = len(docs)\n",
    "    for word in words:\n",
    "        word = word.replace('.','')\n",
    "        if word not in whisky_stopwords:\n",
    "            if word in idf:\n",
    "                idf[word] = countDocs(word, docs)\n",
    "            else:\n",
    "                idf[word] = 1\n",
    "    idf.update(\n",
    "        (x, math.log(int(n_docs)/y)) for x, y in idf.items()\n",
    "    )\n",
    "    return idf\n",
    "\n",
    "def tf_idf(corpus, docs):\n",
    "    tf = wordTFScore(corpus, docs)\n",
    "    idf = wordIDFScore(corpus, docs)\n",
    "    tf_idf = [\n",
    "        (word, tf[word]*idf[word]) for word in tf.keys()\n",
    "    ]\n",
    "    tf_idf.sort(key=itemgetter(1), reverse=True)\n",
    "    return tf_idf\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-savings",
   "metadata": {},
   "source": [
    "### No Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "entire-arthur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1148.445481300354 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "tf_idf_wrds = tf_idf(corp, lst)\n",
    "\n",
    "t2 = time.time()\n",
    "tf_idf_time = t2 - t1\n",
    "print(f\"Time taken: {tf_idf_time} seconds\")\n",
    "\n",
    "results[\"tf_idf\"] = {}\n",
    "results[\"tf_idf\"][\"kws\"] = tf_idf_wrds\n",
    "results[\"tf_idf\"][\"time\"] = tf_idf_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-guatemala",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "driven-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "# Wordnet doesn't have an implementation of a cache, meaning it needs to query wordnet \n",
    "# each time.  Adding in a cache to save time.\n",
    "wordnet_cache = {}\n",
    "def replaceWithLemmas(txt):\n",
    "    split = txt.split()\n",
    "    lemma_txt = ''\n",
    "    for word in split:\n",
    "        if word not in whisky_stopwords:\n",
    "            if word in wordnet_cache:\n",
    "                lemma = wordnet_cache[word]\n",
    "            else:\n",
    "                # Getting POS tag\n",
    "                tag = pos_tag([word])[0][1][0].lower()\n",
    "                if tag == \"v\":\n",
    "                    tag =  \"v\"\n",
    "                if tag == \"j\":\n",
    "                    tag =  \"a\"\n",
    "                else:\n",
    "                    tag = \"n\"\n",
    "                lemma = wordnet.lemmatize(word, tag)\n",
    "            lemma_txt = lemma_txt + ' ' + lemma\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "immune-satin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 125.97502851486206 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "wordnet_lst = [replaceWithLemmas(txt) for txt in lst]\n",
    "wordnet_corp = GraphKE().makeCorpus(wordnet_lst)\n",
    "t2 = time.time()\n",
    "wordnet_lematizing = t2-t1\n",
    "results[\"wordnet_lemmatizing_time\"] = wordnet_lematizing\n",
    "print(f\"Time taken: {wordnet_lematizing} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unsigned-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1204.9405109882355 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "\n",
    "wordnet_tf_idf = tf_idf(wordnet_corp, wordnet_lst)\n",
    "\n",
    "t2 = time.time()\n",
    "wordnet_tf_idf_time = t2 - t1\n",
    "print(f\"Time taken: {wordnet_tf_idf_time} seconds\")\n",
    "\n",
    "results[\"wordnet_tf_idf\"] = {}\n",
    "results[\"wordnet_tf_idf\"][\"kws\"] = wordnet_tf_idf\n",
    "results[\"wordnet_tf_idf\"][\"time\"] = wordnet_tf_idf_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-booth",
   "metadata": {},
   "source": [
    "### Custom Whisky Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conceptual-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 4.4477245807647705 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "whiskylemmatizer = WhiskyLemmatizer()\n",
    "whisky_lst = [\n",
    "    \" \".join(whiskylemmatizer.tokenFilter(text)) for text in lst \n",
    "]\n",
    "whisky_corp = GraphKE().makeCorpus(whisky_lst)\n",
    "t2 = time.time()\n",
    "whisky_lemmatizing_time = t2 - t1\n",
    "results[\"whisky_lemmatizing_time\"] = whisky_lemmatizing_time\n",
    "print(f\"Time taken : {whisky_lemmatizing_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cutting-louisiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1166.1536271572113 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "whisky_tf_idf = tf_idf(whisky_corp, whisky_lst)\n",
    "\n",
    "t2 = time.time()\n",
    "whisky_tf_idf_time = t2 - t1\n",
    "print(f\"Time taken: {whisky_tf_idf_time} seconds\")\n",
    "\n",
    "results[\"whisky_tf_idf\"] = {}\n",
    "results[\"whisky_tf_idf\"][\"kws\"] = whisky_tf_idf\n",
    "results[\"whisky_tf_idf\"][\"time\"] = whisky_tf_idf_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-elite",
   "metadata": {},
   "source": [
    "## RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "unique-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake, Metric\n",
    "def rakeAsList(corpus):\n",
    "    raker = Rake(corpus, min_length=1, max_length=1)\n",
    "    raker.extract_keywords_from_text(corpus)\n",
    "    return raker.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-collection",
   "metadata": {},
   "source": [
    "### Unlemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "protected-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.44502973556518555 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "unlemma_rake = rakeAsList(corp)\n",
    "\n",
    "t2 = time.time()\n",
    "unlemma_rake_time = t2 - t1\n",
    "print(f\"Time taken: {unlemma_rake_time} seconds\")\n",
    "\n",
    "results[\"unlemma_rake\"] = {}\n",
    "results[\"unlemma_rake\"][\"kws\"] = unlemma_rake\n",
    "results[\"unlemma_rake\"][\"time\"] = unlemma_rake_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-deputy",
   "metadata": {},
   "source": [
    "### Wordnet Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "designing-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.46202850341796875 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "wordnet_rake = rakeAsList(wordnet_corp)\n",
    "\n",
    "t2 = time.time()\n",
    "wordnet_rake_time = t2 - t1\n",
    "print(f\"Time taken: {wordnet_rake_time} seconds\")\n",
    "\n",
    "results[\"wordnet_rake\"] = {}\n",
    "results[\"wordnet_rake\"][\"kws\"] = wordnet_rake\n",
    "results[\"wordnet_rake\"][\"time\"] = wordnet_rake_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-barcelona",
   "metadata": {},
   "source": [
    "### Whisky Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "large-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.2689990997314453 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "whisky_rake = rakeAsList(whisky_corp)\n",
    "\n",
    "t2 = time.time()\n",
    "whisky_rake_time = t2 - t1\n",
    "print(f\"Time taken: {whisky_rake_time} seconds\")\n",
    "\n",
    "results[\"whisky_rake\"] = {}\n",
    "results[\"whisky_rake\"][\"kws\"] = whisky_rake\n",
    "results[\"whisky_rake\"][\"time\"] = whisky_rake_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-positive",
   "metadata": {},
   "source": [
    "## Eigencentrality RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "advisory-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Corpus\n",
      "Building Graph\n",
      "Candidate Keywords Selected\n",
      "Edges Created\n",
      "Ranking Nodes\n",
      "Time taken: 44.06533336639404 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "erake = GraphKE().keywordExtract(df, \"All\")\n",
    "\n",
    "t2 = time.time()\n",
    "erake_time = t2 - t1\n",
    "print(f\"Time taken: {erake_time} seconds\")\n",
    "\n",
    "results[\"erake\"] = {}\n",
    "results[\"erake\"][\"kws\"] = erake\n",
    "results[\"erake\"][\"time\"] = erake_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-happening",
   "metadata": {},
   "source": [
    "## Saving Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "every-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
