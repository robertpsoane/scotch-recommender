\section{Approach}\label{sec:approach}
\subsection{Requirements}
The following broad requirements were defined:
\begin{itemize}
    \item \textbf{Agent and Environment} - The recommender should be an agent acting in an environment. 
    The environment being the contents of the Master of Malt (MoM) website, and an interface with a user.
    The agent could be considered as part of a backend of a web app with outputs in a Python dictionary/JSON
    format.\footnote{\textbf{Note:} The webapp was beyond the scope of this project}
    \item \textbf{Speed} - The agent should be able to recommend
    within a couple of seconds.
    \item \textbf{Customisable} - An end user should be able to filter by price,
    volume and ABV.
    \item \textbf{Updateable} - The agent should be able to automatically update
    it's database, and retrain its models.
    \item \textbf{Input Types} - The agents should recommend based on \emph{likes} 
    \& \emph{dislikes} of whiskies supplied by a user, or from a user's written 
    tasting notes.
\end{itemize}

\subsection{The Data}
In order to build a whisky specific language model (discussed further in
\autoref{whiskylang}), a large corpus of tasting notes was required.

Product data for a large range of scotch whiskies was scraped from
\href{http://masterofmalt.com/}{masterofmalt.com}, this dataset contains a selection of attributes for each 
whisky.\footnote{\href{http://masterofmalt.com/}{masterofmalt.com} is a major UK whisky and spirits retailer.}$^{,}$\footnote{The scraping process is discussed in \autoref{sssec:scrape}}
Names and URLs are hashed together using MD5 to provide an ID.

It was observed that whiskies which are discontinued tend to be listed
without a price, whereas those which are out of stock are listed with a
price.  For this reason, simplicity's sake, price was taken as an
indication of stock.  Those without a price
were still recorded for two reasons; users may wish to make recommendations
based on liking or disliking them, and they add to the corpus of tasting notes.

\subsection{Choice of Recommender Method}
While an online spirits shop may use CF recommenders to recommend products based on those being viewed
(such as recommending gin to customers who like gin etc.), a CF system may fail for
recommending whiskies based on tastes.

It is not unlikely that a whisky drinker may wish to buy two very different whiskies in one order, just 
to compare them, or they might enjoy a large range of whiskies. One might like a large
variety of whiskies, and purchase many different styles frequently. 
It seems unlikely that the shopping habits of whisky enthusiasts is sufficient to recommend a whisky
based on specific tastes.

For that reason, a CB recommender model was chosen, using tasting note data.

\subsection{NLP Methods}\label{whiskylang}
Word2vec and BoW were both considered as language models. While there are
many pretrained models available, these are likely unsuitable due to whisky's lexicon.

Word2vec encapsulates far more semantic data, however re-training
word2vec regularly with new data would be expensive. As a quicker model to train
BoW was chosen.  TF-IDF, RAKE and an eigencentrality ranking measure discussed in 
\autoref{ssec:erake} were considered for KE.

\subsubsection{The Ideal Vector and Similarity}\label{sssec:cossim}
BoW maps each input to a vector. To make recommendations, the agent must
map user input to a vector in the same space as the BoW model. This
\emph{Ideal Vector} (IV) represents a hypothetical whisky which best represents 
the input. Cosine similarity can be used to ascertain which whiskies in 
the database best match the input.  Cosine similarity indicates the angle between
vectors \cite{Melville2010}. As  
\begin{equation}
    \utilde{u} \cdot \utilde{v} \coloneqq \vert \utilde{u} \vert \vert \utilde{v} \vert \cos{\theta}
\end{equation}
for $\utilde{u}, \utilde{v}\in \mathbb{R} ^{k}$, by keeping all vectors normalised, this reduces such
that the cosine similarity of $\utilde{u}$ \& $\utilde{v}$ is their scalar product.

Calculating cosine similarity for a large dataset is straightforward.
Consider our dataset of $m$ whiskies as
\begin{equation}
    \textbf{D} \in \mathbb{R}^{m \times n}
\end{equation}
with each row representing the corresponding whisky's vector, and our IV $\utilde{v} \in \mathbb{R}^{n}$.
The product of 
\begin{equation}
    \textbf{D} \cdot \utilde{v} = \utilde{c}    
\end{equation}
or 
\begin{equation}\label{eqn:cossim}
    \begin{pmatrix}
        d_{11} & d_{12} & ... & d_{1n}\\
        d_{21} & d_{22} & ... & d_{2n}\\
        ...    & ...    & ^{\cdot}\cdot _{\cdot} & ...   \\
        d_{m1} & d_{m2} & ... & d_{mn}
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        v_1 \\ v_2 \\ v_3 \\ ... \\ v_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        c_1 \\ c_2 \\ c_3 \\ ... \\ c_n
    \end{pmatrix}
\end{equation}
gives cosine similarities between IV and each whisky in $D$, where $\utilde{c}$ is our vector of cosine similarities.




\subsubsection{Tasting Notes}\label{ssec:tnotes}
As shown in table \ref{tab:tnotes}, whisky tasting notes are keyword dense.\footnote{All table
data from \href{http://masterofmalt.com/}{masterofmalt.com}}
Most words are candidate keywords, however some KE techniques (such as RAKE) aim to 
find keywords from far less keyword dense text. This must be considered when choosing a KE method.

\begin{table}
    \centering
    \caption{A selection of whisky tasting notes from Master of Malt}\label{tab:tnotes}
    \begin{tabular}{p{0.2\linewidth} p{0.8\linewidth}} 
        \toprule
        Whisky                                    & Tasting Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\
        \midrule
              Laphroaig 10 Year Old                     & 
        \begin{minipage}[t]{0.8\columnwidth}\textbf{Nose:}
            \textit{~``This opens on big, smoky muscular peat notes. There are
             spices, and liquorice, as well as a big dose of salt. This whisky has become slightly sweeter in recent years,
              and it appears beautifully on the nose, amidst the classic iodine/sticking plasters and cool wood smoke we
               love.''}\\
               \textbf{Palate:}\textit{~``Seaweed-led, with a hint of vanilla ice cream and more than a whiff of notes
                from the first aid box (TCP, plasters etc). The oak is big, and muscles its way into the fore as you hold 
                this whisky over your tongue. An upsurge of spices develop â€“ cardamom/black pepper/chilli.''
                }\\
                \textbf{Finish:}\textit{~``Big and drying, as the savoury, tarry notes build up with an iodine complexity.''}\end{minipage}  \\
                \midrule
                Talisker 10 Year Old                      & 
        \begin{minipage}[t]{0.8\columnwidth}\textbf{Nose:}
            \textit{~``A fresh and fragrant nose. Through thick, pungent 
            smoke comes sweet pear and apple peels, with pinches of maritime salt from kippers, seaweed.''
            }\\
            \textbf{Palate:}\textit{~``It's a bonfire of peat crackling with black pepper, with a touch of brine 
            and dry barley. A welcome delivery of orchard fruit provides a delicate and beautiful balance.''
            }\\
            \textbf{Finish:}~\textit{``In a long finish, bonfire embers toast malt and crystallise a sugary underlay''
            }\end{minipage}                                                                                                                                                                                                                                                    \\
        \toprule
    \end{tabular}
\end{table}

\subsubsection{Eigencentrality based Rapid Automatic Keyword Extraction (eRAKE)}\label{ssec:erake}
As discussed in section \ref{sssec:gbkwe}, co-occurrence graphs can be useful for KE.  RAKE is one such method where primitive
centrality measures are used to rank nodes.  Another method uses eigencentrality.  This steps beyond words which
themselves have a high co-occurrence and rewards words with significantly edges to words with high co-occurrences.
This could be a compromise for retaining semantic data.  While losing full semantic data we are
at least stepping beyond merely looking at frequencies, selecting descriptors with larger amounts of influence 
across the dataset.

As the graph is undirected, our adjacency matrix is hermitian and thus finding it's eigenvectors is trivial,
SciPy's $eigh()$ function can be used \cite{hubbard_2020, 2020NumPy}.