\section{Background and Literature}\label{sec:lit}

\subsection{Language Models}\label{ssec:kwe}
In general, Natural Language Processing (NLP) tasks require a language model of some form or another.
Artificial Intelligence (AI) based methods cannot process text in its native unstructured form, but need to convert the
raw text to a structured form suitable for the computer to understand.

The two predominant model types are \emph{syntactic} and \emph{semantic} models. 
Syntactic methods transform text to a set of `symbols' which carry no inherent meaning, but can be 
compared across instances in a dataset, wheras semantic methods (such as those described in \autoref{ssec:w2v})
retain a general understanding of the text \cite{Cambria2014}.

A dominant syntactic method for transforming unstructured text into a computer-analysable form is the Bag-of-words 
(BoW) model. 
The dataset is tokenized (split into individual words), lemmatized (see \autoref{sssec:stemlemma})
and $k$ keywords are extracted (see \autoref{sssec:kwe}) to form our bag of words $b\in \mathbb{R} ^{k}$.
Each instance in the dataset is now transformed to a vector $v\in \mathbb{R} ^{k}$ such that $v_i$ is the frequency 
of the word $b_i$ occuring in the instance \cite{Cambria2014, StevenBirdEwanKlein2009, Zhang2010}.

\subsubsection{Stemming and Lemmatization}\label{sssec:stemlemma}
When dealing with text data, it is not uncommon to have multiple forms of the same word.  A syntactic model would view
the words `cat' and `cats' as two different discrete symbols.  A method is needed to reduce words to a normal form.

Porter proposed an algorithm for removing word suffixes to aim for a normal form, this is called
\emph{stemming}. With no semantic understanding, the algorithm searches for specific suffix patterns and removes them
until it is unable to \cite{Porter1980}.

A semantic approach would be \emph{lemmatization}. Instead of algorithmically removing word endings lemmatization 
aims to normalise words to a real word root, that is a lemmatizer would reduce the word to the dictionary
form of the word \cite{Jayakodi2016}.  A lemmatizer implementation in Python is the WordNetLemmatizer in the Python 
Natural Language Tool Kit (NLTK), which queries the WordNet corpus to find the root word 
\cite{StevenBirdEwanKlein2009, princetonuniversity_2010}. 
 
\subsubsection{Keyword Extraction}\label{sssec:kwe}
For syntactic methods, keyword extraction is key.  
\subsubsection{TF-IDF}\label{ssec:tfidf}
\subsubsection{Graph based keyword extraction}\label{sssec:gbkwe}

\subsubsection{Word2vec}\label{ssec:w2v}
\subsection{Recommender engines}\label{ssec:recommenders}
\subsubsection{Collaborative filters}\label{sssec:collab}
\subsubsection{Content Driven Recommendations}\label{sssec:content}

\subsection{Machine learning applications to Whisky}\label{ssec:ml2whisk}


