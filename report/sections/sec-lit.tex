\section{Background and Literature}\label{sec:lit}

\subsection{Language Models}\label{ssec:kwe}
In general, Natural Language Processing (NLP) tasks require a language model of some form or another.
Artificial Intelligence (AI) based methods cannot process text in its native unstructured form, but need to convert the
raw text to a structured form suitable for the computer to understand.

The two predominant model types are \emph{syntactic} and \emph{semantic} models. 
Syntactic methods transform text to a set of `symbols' which carry no inherent meaning, but can be 
compared across instances in a dataset, wheras semantic methods (such as those described in \autoref{ssec:w2v})
retain a general understanding of the text \cite{Cambria2014}.

A dominant syntactic method for transforming unstructured text into a computer-analysable form is the Bag-of-words 
(BoW) model. 
The dataset is tokenized (split into individual words), lemmatized (see \autoref{sssec:stemlemma})
and $k$ keywords are extracted (see \autoref{sssec:kwe}) to form our bag of words $b\in \mathbb{R} ^{k}$.
Each document\footnote{It is common to refer to an instance in a text dataset as a \emph{document}} is
transformed to a vector $v\in \mathbb{R} ^{k}$ such that $v_i$ is the frequency 
of the word $b_i$ occuring in the document \cite{Cambria2014, StevenBirdEwanKlein2009, Zhang2010}.

\subsubsection{Stemming and Lemmatization}\label{sssec:stemlemma}
When dealing with text data, it is not uncommon to have multiple forms of the same word.  A syntactic model would view
the words `cat' and `cats' as two different discrete symbols.  A method is needed to reduce words to a normal form.

Porter proposed an algorithm for removing word suffixes to aim for a normal form, this is called
\emph{stemming}. With no semantic understanding, the algorithm searches for specific suffix patterns and removes them
until it is unable to \cite{Porter1980}.

A more semantic approach would be \emph{lemmatization}. Instead of algorithmically removing word endings lemmatization 
aims to normalise words to a real word root, that is a lemmatizer would reduce the word to the dictionary
form of the word \cite{Jayakodi2016}.  A lemmatizer implementation in Python is the WordNetLemmatizer in the Python 
Natural Language Tool Kit (NLTK), which queries the WordNet corpus to find the root word 
\cite{StevenBirdEwanKlein2009, princetonuniversity_2010}. 
 
\subsubsection{Keyword Extraction}\label{sssec:kwe}
For syntactic methods, keyword extraction is key.  For the purposes of this report, a keyword is a word of particular
relevance or importance, and from which we might extract useful information.  Keyword extraction refers to strategies
based on which those important words can be ranked, and only the most relevant kept.

\paragraph{TF-IDF}\label{ssec:tfidf}

Onesuch method, is Term Frequency Inverse Document Frequency (TF-IDF). This is commonly used with BoW, and is 
implemented in Scikit-Learn \cite{Barupal2011}.  TF-IDF is a statistic for scoring a words importance based the word's
how frequently it occurs in a document, and how frequently it occurs in the dataset \cite{Ramos2003}.

Scoring as such aims to penalise words that occur too frequently across a document, boosting the scores of words in an 
individual document for which they with disproportionately high frequency.

\paragraph{Graph based keyword extraction}\label{sssec:gbkwe}

\subsubsection{Word2vec}\label{ssec:w2v}
\subsection{Recommender engines}\label{ssec:recommenders}
\subsubsection{Collaborative filters}\label{sssec:collab}
\subsubsection{Content Driven Recommendations}\label{sssec:content}

\subsection{Machine learning applications to Whisky}\label{ssec:ml2whisk}


