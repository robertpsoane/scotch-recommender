\section{Background and Literature}\label{sec:lit}

\subsection{Language Models}\label{ssec:kwe}
In general, Natural Language Processing (NLP) tasks require a language model of some form or another.
Artificial Intelligence (AI) based methods cannot process text in its native unstructured form, but need to convert the
raw text to a structured form suitable for computer understanding. This is often referred to as \emph{embedding}.

The two dominant model types are \emph{syntactic} and \emph{semantic} models. 
Syntactic models transform text to a set of `symbols' which carry no inherent meaning, but can be 
compared across instances in a dataset, whereas semantic methods (such as those described in \autoref{ssec:w2v})
retain a contextual understanding of text \cite{Cambria2014}.

A dominant syntactic method for transforming unstructured text into a computer-analysable form is the Bag-of-words 
(BoW) model. 
The dataset is tokenized (split into individual words), lemmatized (see \autoref{sssec:stemlemma})
and $k$ keywords are extracted (see \autoref{sssec:kwe}) to form our bag of words $\utilde{b}\in \mathbb{R} ^{k}$.
Each document is
transformed to a vector $\utilde{v}\in \mathbb{R} ^{k}$ such that $v_i$ is the frequency 
of the word $b_i$ occuring in the document \cite{Cambria2014, StevenBirdEwanKlein2009, Zhang2010}.\footnote{It is common to refer to an instance in a text dataset as a \emph{document}}

\subsubsection{Stemming and Lemmatization}\label{sssec:stemlemma}
When dealing with text data, it is not uncommon to have multiple forms of the same word.  A syntactic model would view
the words `cat' and `cats' as two different discrete symbols.  A method is needed to reduce words to a normal form.

Porter \cite{Porter1980} proposed an algorithm for removing word suffixes to aim for a normal form, this is called
\emph{stemming}. With no semantic understanding, the algorithm matches specific suffix patterns and removes them
until it is unable to.

A more semantic approach would be \emph{lemmatization}. Instead of algorithmically removing word endings, lemmatization 
normalises words to a real word root - the dictionary
form of the word \cite{Jayakodi2016}.  One lemmatizer implementation in Python is the WordNetLemmatizer in Python's
Natural Language Tool Kit (NLTK), which queries the WordNet corpus to find the root word 
\cite{StevenBirdEwanKlein2009, princetonuniversity_2010}. 
 
\subsubsection{Keyword Extraction}\label{sssec:kwe}
For syntactic methods, keyword extraction (KE) is key.  For the purposes of this report, a keyword is a word of particular
relevance or importance, and from which we might extract useful information.  KE refers to strategies
based on which those important words can be ranked, keeping only the most relevant.

\paragraph{TF-IDF}\label{ssec:tfidf}

Onesuch method, is Term Frequency Inverse Document Frequency (TF-IDF). This is commonly used with BoW, and is 
implemented in Scikit-Learn \cite{Barupal2011}.  TF-IDF is a statistic for scoring a words importance based on
how frequently they occur in a document, and in the dataset \cite{Ramos2003}.

Scoring as such aims to penalise words that occur too frequently across a document, boosting scores of words in an 
individual document which appear with disproportionately high frequency.

\paragraph{Graph based KE}\label{sssec:gbkwe}

Another approach for KE is the use of graph-based ranking methods.  These methods model words as
nodes on a mathematical network graph.\footnote{A graph $G$ being a set of nodes $V$ and edges $E$.  For a brief 
summary see Rashid Bin Muhammad's site http://personal.kent.edu/~rmuhamma/GraphTheory/MyGraphTheory/defEx.htm
\cite{muhammad}.}  A popular example is \emph{Rapid Automatic Keyword Extraction} (RAKE), which
finds a set of candidate keywords, and models them as a co-occurence graph.  
Each node represents a candidate, each edge co-occurence, and it's weight the number of co-occurences.  
Candidates are ranked according to frequency and degree~\cite{Rose2010}.

Beliga et al. \cite{Beliga2015}, survey a wide range of graph based KE techniques, many of which rely on different 
centrality measures . Onesuch centrality measure is 
eigencentrality \cite{Bonacich2007}. 
Eigencentrality scores each node as a proportion of the sum of the scores of all nodes to which it is connected. 
Suppose we have a graph, with an adjacency matrix $A$, we would set
\begin{equation}
    x_i = \frac{1}{\lambda}A_{ij}x_j
\end{equation}
using the summation convention, where $x_i$ is the centrality of the $i^{th}$ node. This reduces to the eigenvector equation 
\begin{equation}
    \textbf{A}\cdot \utilde{x} = \lambda \utilde{x}
\end{equation} This is given with more detail in \cite{Newman2010}.

\subsubsection{Word2vec}\label{ssec:w2v}
\emph{Word2vec} is a semantic language model developed by Google.  Instead of encoding each word as a discrete symbol
as with BoW, word2vec embeddings retain similarity between similar words. This is achieved by training an \emph{Artificial 
Neural Network} (ANN) to predict surrounding words for each given word.  The hidden layer's weights represent
probabilities of respective surrounding words. These probability vectors are used as embeddings for each word. As a words
embedding now reflects likely surrounding words, synonyms are mapped to similar vectors.
\cite{Mikolov2013, McCormick2017, Liu2020}

\subsection{Recommender Agents}\label{ssec:recommenders}
Collaborative filtering (CF) is perhaps the most common recommender engine method. CF treats each user as an entity, 
and provides recommendations to users based on the behaviours of users it considers similar 
\cite{Melville2010, Herlocker2000}. A simple, less abstract, example would be an online shopping site recommending
product $B$ to someone who has bought product $A$ on the basis that a significant proportion of shoppers who buy
product $A$ go on to purchase $B$.  It produces a simple filter, making predictions with no product knowledge,
just user patterns.

Content based (CB) recommender engines are the opposite.  Instead of focussing on user patterns they make predictions
on the basis of specific attributes of each entity being recommended \cite{Melville2010, Mooney2000}.  While such a 
system may use user details, the main knowledge source is the entities being recommended.

\subsection{AI applications to Whisky}\label{ssec:ml2whisk}

There is a large gap in the research regarding Artificial Intelligence (AI) applications to whisky.
Coldevin built an agent based whisky recommender, choosing to use a CB design \cite{Coldevin2005}.  He 
built a system using specific attributes about the whisky to recommend based
on consumer likes or dislikes. Omid-Zohoor and Eghtesadi built another such hybrid (using both CB and CF) 
agent \cite{Omidzohoor}, however again they relied on specific categorical and ratio features.
An interesting design choice was to recommend on the basis of a user's entire profile, and the 
ratings they give.  Perhaps unsurprisingly their CB model performed poorly with users who gave large 
numbers of reviews. The more reviews the more noise.

I think that an agent would be far more effective if it took a set of specific user preferences,
at a given point in time, and makes a recommendation on that basis.  Instead of attempting
to offer users a whisky which best matches all varied whiskies they like, users get
recommendations of the style they want at the time.  This is far 
closer to Coldevin's approach.

Wishart completed what seems to be the only study of whisky which uses NLP \cite{Wishart2000}.
Working with industry experts he selected 84 different whiskies and extracted descriptors from their tasting notes. 
These were coded and used to cluster the whiskies. These clusters were manually reviewed and evaluated by 
industry experts.  He later proposed a set of 12 flavour dimensions for Scotch whisky \cite{Wishart2009}.

While groundbreaking, Whisharts work differs somewhat from that carried out in this project.  Where Wishart aimed to find
key flavours in Scotch, working with industry experts, I aim to produce an agent which exhibits intelligence as an 
industry expert by suggesting whiskies, based only on their tasting notes.

