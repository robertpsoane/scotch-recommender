@article{Ranjan2016,
abstract = {The field of natural language processing (aka NLP) is an intersection of the study of linguistics, computation and statistics. The primary goal of NLP is automated understanding of the semi-structured language that humans use. This study stems application in diverse fields like semantic analysis, summarization, text classification and the like. The domain natural language processing is a fledgling domain with no concrete indication of when it will mature. Compared to well established domains like "Study of Algorithms", NLP is yet in its emerging period and hence there's dearth of a concise piece of literature that elaborates on the phases of NLP and lists different techniques that can be adapted. NLP borrows heavily from foundational subjects of study like statistics, probability theory and theory of computation. In this paper, we describe three phases of natural language processing namely, language modelling, parts-of-speech tagging and parsing, outlining the approaches used that can be used.},
author = {Ranjan, Nihar and Mundada, Kaushal and Phaltane, Kunal and Ahmad, Saim},
doi = {10.5120/ijca2016907355},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Ranjan et al/International Journal of Computer Applications/Ranjan et al. - 2016 - A Survey on Techniques in NLP.pdf:pdf},
journal = {International Journal of Computer Applications},
mendeley-groups = {NLP},
number = {8},
pages = {6--9},
title = {{A Survey on Techniques in NLP}},
volume = {134},
year = {2016}
}
@book{Jacques2003,
abstract = {Lignocellulosic materials have a very complex configuration that contains a variety of active sites capable, in some cases, of adsorbing contaminants from water. Agave bagasse is a sub-product from the alcohol industry that has been very little studied, but that could have the potential to remove a variety of contaminants from aqueous solutions.Raw and modified Agave salmiana bagasse were characterized, before and after they were tested to remove metal cations, by acid-base titrations, elemental analysis and ATR-FTIR. HCl, HNO 3, NaOH, tartaric, citric and oxalic acids were used to modify bagasse to determine if its concentration of active groups could be improved. These materials were then tested for the removal of Cd(II), Pb(II) and Zn(II) ions from water at pH 5, and desorption studies were performed at pH 2 and 4 at 25??C.The characterization techniques mainly identified carboxyl, hydroxyl, sulfur and nitrogen containing groups in bagasse. It was clear that mainly the carboxylic groups were responsible for metal uptake. Raw bagasse has an adsorption capacity of about 8, 14 and 36mgg -1 for zinc, cadmium and lead, respectively, and this was improved about 27-62{\%} upon modification with HNO 3 and NaOH. Treatments with citric, oxalic and tartaric acid did not have a significant effect in adsorption capacity.Raw agave bagasse has a very acceptable adsorption capacity of metal cations and it can approximately be regenerated in a 45{\%}, since the biosorption mechanism involves ion exchange and complexation. ?? 2012 Elsevier B.V.},
author = {Jacques, K.A. and Lyons, T.P. and Kelsall, D.R.},
booktitle = {In: Jacques KA, Lyons TP, Kelsall DR (eds) The alcohol textbook},
edition = {4},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Jacques, Lyons, Kelsall/In Jacques KA, Lyons TP, Kelsall DR (eds) The alcohol textbook/Jacques, Lyons, Kelsall - 2003 - The Alcohol Textbook 4th edition.pdf:pdf},
isbn = {1-897676-13-1},
pages = {223--245},
publisher = {Nottingham University Press},
title = {{The Alcohol Textbook 4th edition}},
year = {2003}
}
@article{Pyke1965,
author = {Pyke, Magnus},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Pyke/The Distillers Company Ltd., Glenochil Research Station, Menstrie, Clackmannanshirc, Scotland)/Pyke - 1965 - THE MANUFACTURE OF SCOTCH GRAIN WHISKY.pdf:pdf},
journal = {The Distillers Company Ltd., Glenochil Research Station, Menstrie, Clackmannanshirc, Scotland)},
pages = {209--218},
title = {{THE MANUFACTURE OF SCOTCH GRAIN WHISKY}},
volume = {71},
year = {1965}
}
@article{legislation.gov.uk_2009,
title={The Scotch Whisky Regulations 2009},
url={https://www.legislation.gov.uk/uksi/2009/2890/contents/made},
journal={Legislation.gov.uk},
publisher={Queen's Printer of Acts of Parliament},
year={2009}}
@article{Bathgate2019,
abstract = {Over the last 50 years there has been considerable analytical research on the malt parameters which govern the flavour and aroma, i.e. the ‘character', of Scotch malt whisky. This has led to a standard format, the Flavour Wheel, to describe the attributes of any malt spirit and to relate them to a common group of malt parameters. What has not been well documented in the same period are the changes in malt processing technology which have led to a gradual but significant change in the concentrations of reference compounds used in the Flavour Wheel relating to sulphury, nutty/burnt/smoky, peaty and fruity/estery characters. This review covers the most significant of these changes, which have become common practice across the industry, and specifically demonstrates how different malting barley varieties, malt kilning and peating, wort recovery, yeast management, and wash still heating have all contributed to the loss of some styles of malt whisky. {\textcopyright} 2019 The Institute of Brewing {\&} Distilling.},
author = {Bathgate, George N.},
doi = {10.1002/jib.556},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Bathgate/Unknown/Bathgate - 2019 - The influence of malt and wort processing on spirit character the lost styles of Scotch malt whisky.pdf:pdf},
issn = {20500416},
journal = {Journal of the Institute of Brewing},
keywords = {distilling,flavour,furfural,kilning,malt,malt whisky,peat,phenols,spirit character,wort clarity},
number = {2},
pages = {200--213},
title = {{The influence of malt and wort processing on spirit character: the lost styles of Scotch malt whisky}},
volume = {125},
year = {2019}
}
@article{Valaer1940,
author = {Valaer, Peter},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Valaer/Industrial and Engineering Chemistry/Valaer - 1940 - Scotch Whisky.pdf:pdf},
journal = {Industrial and Engineering Chemistry},
number = {7},
pages = {935--943},
title = {{Scotch Whisky}},
volume = {32},
year = {1940}
}
@article{Smith2017,
abstract = {Background: Although most Scotch whisky is blended from different casks, a firm distinction exists in the minds of consumers and in the marketing of Scotch between single malts and blended whiskies. Consumers are offered cultural, geographical and production reasons to treat Scotch whiskies as falling into the categories of blends and single malts. There are differences in the composition, method of distillation and origin of the two kinds of bottled spirits. But does this category distinction correspond to a perceptual difference detectable by whisky drinkers? Do experts and novices show differences in their perceptual sensitivities to the distinction between blends and single malts? To test the sensory basis of this distinction, we conducted a series of blind tasting experiments in three countries with different levels of familiarity with the blends versus single malts distinction (the UK, the USA and France). In each country, expert and novice participants had to perform a free sorting task on nine whiskies (four blends, four single malts, one single grain, plus one repeat) first by olfaction, then by tasting.},
author = {Smith, Barry C. and Sester, Carole and Ballester, Jordi and Deroy, Ophelia},
doi = {10.1186/s13411-017-0056-x},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Smith et al/Unknown/Smith et al. - 2017 - The perceptual categorisation of blended and single malt Scotch whiskies.pdf:pdf},
issn = {2044-7248},
journal = {Flavour},
number = {1},
pages = {1--9},
publisher = {Flavour},
title = {{The perceptual categorisation of blended and single malt Scotch whiskies}},
volume = {6},
year = {2017}
}
@article{Mosedale1998,
author = {Mosedale, J.R and Puech, J.-L},
doi = {10.1016/S0924-2244(98)00024-7},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Mosedale, Puech/Trends in Food Science {\&} Technology/Mosedale, Puech - 1998 - Wood maturation of distilled beverages.pdf:pdf},
issn = {09242244},
journal = {Trends in Food Science {\&} Technology},
month = {mar},
number = {3},
pages = {95--101},
title = {{Wood maturation of distilled beverages}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0924224498000247},
volume = {9},
year = {1998}
}
@book{StevenBirdEwanKlein2009,
author = {{Steven Bird, Ewan Klein}, Edward Loper},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Steven Bird, Ewan Klein/Unknown/Steven Bird, Ewan Klein - 2009 - Natural Language Processing with Python.pdf:pdf},
isbn = {978-0-596-51649-9},
mendeley-groups = {NLP},
publisher = {O'Reilly Media Inc},
title = {{Natural Language Processing with Python}},
year = {2009}
}
@article{Cambria2014,
abstract = {Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of 'jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves-namely Syntactics, Semantics, and Pragmatics Curves- which will eventually lead NLP research to evolve into natural language understanding.},
author = {Cambria, Erik and White, Bebo},
doi = {10.1109/MCI.2014.2307227},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Cambria, White/IEEE Computational Intelligence Magazine/Cambria, White - 2014 - Jumping NLP curves A review of natural language processing research.pdf:pdf},
issn = {1556-603X},
journal = {IEEE Computational Intelligence Magazine},
mendeley-groups = {NLP},
month = {may},
number = {2},
pages = {48--57},
publisher = {IEEE},
title = {{Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]}},
url = {http://ieeexplore.ieee.org/document/6786458/},
volume = {9},
year = {2014}
}
@article{Zhang2010,
abstract = {The bag-of-words model is one of the most popular representation methods for object categorization. The key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. For this purpose, a clustering algorithm (e.g., K-means), is generally used for generating the visual words. Although a number of studies have shown encouraging results of the bag-of-words representation for object categorization, theoretical studies on properties of the bag-of-words model is almost untouched, possibly due to the difficulty introduced by using a heuristic clustering process. In this paper, we present a statistical framework which generalizes the bag-of-words representation. In this framework, the visual words are generated by a statistical process rather than using a clustering algorithm, while the empirical performance is competitive to clustering-based method. A theoretical analysis based on statistical consistency is presented for the proposed framework. Moreover, based on the framework we developed two algorithms which do not rely on clustering, while achieving competitive performance in object categorization when compared to clustering-based bag-ofwords representations. {\textcopyright} Springer-Verlag 2010.},
author = {Zhang, Yin and Jin, Rong and Zhou, Zhi Hua},
doi = {10.1007/s13042-010-0001-0},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Zhang, Jin, Zhou/International Journal of Machine Learning and Cybernetics/Zhang, Jin, Zhou - 2010 - Understanding bag-of-words model A statistical framework.pdf:pdf},
issn = {18688071},
journal = {International Journal of Machine Learning and Cybernetics},
keywords = {Bag of words model,Object recognition,Rademacher complexity},
mendeley-groups = {NLP},
number = {1-4},
pages = {43--52},
title = {{Understanding bag-of-words model: A statistical framework}},
volume = {1},
year = {2010}
}
@article{Plisson2004,
abstract = {Lemmatization is the process of finding the normalized form of a word. It is the same as looking for a transformation to apply on a word to get its normalized form. The approach presented in this paper focuses on word endings: what word suffix should be removed and/or added to get the normalized form. This paper compares the results of two word lemmatization algorithms, one based on if-then rules and the other based on ripple down rules induction algorithms. It presents the problem of lemmatization of words from Slovene free text and explains why the Ripple Down Rules (RDR) approach is very well suited for the task. When learning from a corpus of lemmatized Slovene words the RDR approach results in easy to understand rules of improved classification accuracy compared to the results of rule learning achieved in previous work},
author = {Plisson, Jo{\"{e}}l and Lavrac, Nada and Mladeni{\'{c}}, Dr. Dunja},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Plisson, Lavrac, Mladeni{\'{c}}/Proceedings of the 7th International Multiconference Information Society (IS'04)/Plisson, Lavrac, Mladeni{\'{c}} - 2004 - A rule based approach to word lemmatization.pdf:pdf},
journal = {Proceedings of the 7th International Multiconference Information Society (IS'04)},
keywords = {Information Retrieval {\&} Textual Information Access,Natural Language Processing},
mendeley-groups = {Whisky Classification},
number = {November},
pages = {83--86},
title = {{A rule based approach to word lemmatization}},
url = {http://eprints.pascal-network.org/archive/00000715/},
year = {2004}
}
@article{Porter1980,
author = {Porter, M.F.},
doi = {10.1108/eb046814},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Porter/Program/Porter - 1980 - An algorithm for suffix stripping.pdf:pdf},
issn = {0033-0337},
journal = {Program},
month = {mar},
number = {3},
pages = {130--137},
title = {{An algorithm for suffix stripping}},
url = {https://www.emerald.com/insight/content/doi/10.1108/eb046814/full/html},
volume = {14},
year = {1980}
}
@article{Jayakodi2016,
abstract = {Assessment usually plays an indispensable role in the education and it is the prime indicator of student learning achievement. Exam questions are the main form of assessment used in learning. Setting appropriate exam questions to achieve the desired outcome of the course is a challenging work for the examiner. Therefore this research is mainly focused to categorize the exam questions automatically into its learning levels using Bloom's taxonomy. Natural Language Processing (NLP) techniques such as tokenization, stop word removal, lemmatization and tagging were used prior to generating the rule set to be used for this classification. WordNet similarity algorithms with NLTK and cosine similarity algorithm were developed to generate a unique set of rules to identify the question category and the weight for each exam question according to Bloom's taxonomy. These derived rules make it easy to analyze the exam questions. Evaluators can redesign their exam papers based on the outcome of this classification process. A sample of examination questions of the Department of Computing and Information Systems, Wayamba University, Sri Lanka was used for the evaluation; weight assignment was done based on the total value generated from both WordNet algorithm and the cosine algorithm. Identified question categories were confirmed by a domain expert. The generated rule set indicated over 70{\%} accuracy.},
author = {Jayakodi, K. and Bandara, M. and Perera, I. and Meedeniya, D.},
doi = {10.3991/ijet.v11i04.5654},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Jayakodi et al/Unknown/Jayakodi et al. - 2016 - WordNet and Cosine Similarity based Classifier of Exam Questions using Bloom ' s Taxonomy.pdf:pdf},
issn = {18630383},
journal = {International Journal of Emerging Technologies in Learning},
keywords = {Bloom's taxonomy,Cosine similarity,Learning Analytics,Natural Language Processing,Question classification,Teaching and Supporting Learning},
number = {4},
pages = {142--149},
title = {{WordNet and cosine similarity based classifier of exam questions using bloom's taxonomy}},
volume = {11},
year = {2016}
}
@misc{princetonuniversity_2010,
title={What is WordNet?},
url={https://wordnet.princeton.edu/},
journal={Princeton University},
publisher={The Trustees of Princeton University},
year={2010}
}
@article{Ramos2003,
abstract = {In this paper, we examine the results of applying Term Frequency Inverse Document Frequency (TF-IDF) to determine what words in a corpus of documents might be more favorable to use in a query. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in. Words with high TF-IDF numbers imply a strong relationship with the document they appear in, suggesting that if that word were to appear in a query, the document could be of interest to the user. We provide evidence that this simple algorithm efficiently categorizes relevant words that can enhance query retrieval.},
author = {Ramos, Juan},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Ramos/Proceedings of the first instructional conference on machine learning/Ramos - 2003 - Using TF-IDF to Determine Word Relevance in Document Queries.pdf:pdf},
journal = {Proceedings of the first instructional conference on machine learning},
mendeley-groups = {NLP},
number = {1},
pages = {29--48},
title = {{Using TF-IDF to Determine Word Relevance in Document Queries}},
volume = {242},
year = {2003}
}

@article{Barupal2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
author = {Fabian, Pedregosam and Varoquaux, Gael and Gramfort, Alexandre and Vincent, Michel and Thririon, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Mattieu and Duchesnay, Edouard},
file = {:C$\backslash$:/Users/rober/Google Drive/Notes and Books/Mendeley/Fabian et al/Journal of Machine Learning Research/Fabian et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {85},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2011}
}

